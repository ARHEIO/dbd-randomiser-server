# DBD Randomiser Server

The back end for the dead by daylight randomiser application. Works by using serverless to spin up an ecosystem of lambdas. Each lambda independently works with a single instance of the DynamoDB Document Client, and uses various services to access specific tables.

Each lambda is focused around a specific object.

## Setup

### Requires
* Node 12.16
* Docker

### Commands
* `npm run start` will start the sever running so it can be hit on an endpoint. The endpoint exposed is: `http://localhost:9002/dbd-randomiser`

## Endpoints


### Data dictionary
The base path is : `/dbd-randomiser`, and it is assumed that each of the paths below have `/dbd-randomiser` prepended. E.g. `/loadout` becomes `/dbd-randomiser/loadout`.

`:character` is a reference to either `survivor` or `killer`. These are the only values that are supported for the `:character` path.

`:id` is a real positive integer greater than 0. Ids start at 1 in this app.

---
```
/loadout
```
Returns an autogenerated loadout, requires a query param `q` to get a loadout. Supported values for `q` are `survivor` and `killer`. Returns a full loadout for each, sans offerings.

Example: `curl --location --request GET 'https://localhost:9002/dbd-randomiser/loadout?q=killer'`

---
```
/:character
```
Used to get killers or survivors and returns a list of all available. Supported values for `:character` are `killer` and `survivor`.

Example: `curl --location --request GET 'https://localhost:9002/dbd-randomiser/killer`

---
```
/:character/:id
```
Returns a specific character. the `:id` field is a real positive integer (i.e NOT 0). If the value for `:character` is `killer`, then this can be expanded with `?expands=upgradables` to get the addons for that killer.

Example: `curl --location --request GET 'https://localhost:9002/dbd-randomiser/killer/2`

---
```
/perk/:character/
```
Returns a list of all perks available for a certain class of characters. Supported values for `:character` are `killer` and `survivor`.

Example: `curl --location --request GET 'https://localhost:9002/dbd-randomiser/perk/killer`

---
```
/perk/:character/:id
```
Returns a list of all perks available for a certain class of characters. Supported values for `:character` are `killer` and `survivor`.

Example: `curl --location --request GET 'https://localhost:9002/dbd-randomiser/perk/killer/30`

---
```
/item
```
Returns a list of all items available for survivors.

Example: `curl --location --request GET 'https://localhost:9002/dbd-randomiser/item`

---
```
/item/:id
```
Returns a specific item. the `:id` field is a real positive integer (i.e NOT 0). This can be expanded with `?expands=upgradables` to get the addons for that item.

Example: `curl --location --request GET 'https://localhost:9002/dbd-randomiser/item/4`

## Semantics

* The controllers take the response from the lambda and transform the data into parameters to pass down
* Services are orchestrators that will take a request, and based on the parameters passed, generate a request to send to a dynamodb instance
* DB is the data access layer that takes requests to send to dynamodb



## How does this work?
The `/src` directory contains the brains of the actual lmabdas. The entry point for a lambda is contained in the `/src/controllers` folder.
The intention is that each controller will be its own lambda, and each controller will pull in dependencies as needed, and maintains its own orchestration and data manipulation.
The _intention_ is that all the things the pull in are simple functions with no understanding of the surrounding state (with the exception of the DynamoDbClient).

The Webpack build under the `entry` variable, pulls in each specific controller and builds it (and its dependencies) into a single file that can be uploaded. Each controller and its context is then maintained separately, with the only shared code being shared dependencies, minimizing bloat. One controller, one file with all its dependencies included. One thing to upload per lambda.

#### Why go through all that effort?
Firstly, it doesn't make sense to maintain each lambda as a separate repo. The idea is to use lambdas to create a RESTful interface with DynamoDB, and providing the 4 CRUD operations. Mainly because it's cheap; when I'm making things for just me and GitHub, I don't really want to throw money away at running a server. I could've used Lerna to maintain a mono repo, but if they all interact with the same DynamoDB instance, then that's a lot of duplicated code.

Secondly, AWS's own site recommends that when uploading a lambda, that you zip your entire project directory and upload it through the CLI. Here's the proof: https://docs.aws.amazon.com/lambda/latest/dg/nodejs-package.html. That's really dumb. That includes things we don't care about, like build scripts, and docker config, and environment config files,
and every single node module that's a dev dependency. At time of writing, `du -sh .` tells me this directory is 379MB. 

Can you imagine have individual Lambdas that are all 379MB? The _entire_ compiled output at time of writing is 40KB. **40KB**. And that's (currently) split over 3 Lambdas. Smaller lambdas lead to shorter cold starts.

Here's a great article showing that the size of a Lambda effects its coldstart time. https://mikhail.io/serverless/coldstarts/aws/


## How does it work locally?
Through use of Docker and a dummy api gateway.

This app runs off DynamoDB, so we first need an instance of that running. Thankfully there's a docker image running so that's ezpz.

Second it needs the actual lambdas to be running in some environment. https://github.com/lambci/lambci is a package for uploading and deploying Lambdas. However they also provide a docker image where you can specify the handler you want to run, and run it in detached mode. The issue here is that it'll only let you put in one handler. SO by using a router provided by https://github.com/spring-media/aws-lambda-router, we can route requests from one lambda through to the OTHER lambdas. By setting up a separete webpack build for it, it pulls in all the other lambads, making one super lambda. And since we're running it locally, we don't care about cold starts or having one big block of code.

Finally, althrough it all works, the only endpoint exposed is `POST 'http://localhost:9001/2015-03-31/functions/-/invocations'`, and all your path params and query params have to be passed as the body to the object. So we have a way to invoke the function, but it doesn't match our deployed way of interacting with this ecosystem. So with `./docker/api-gateway.js`, we provide a friendly way to replicate AWS's api gateway interface, and transform those requests into lambda events to be properly handled.

## How does deployment work?

Pretty easily actually, we've already spoken about the build up in the "How does this work?" section; once we've got the lambda files output, we specify in the `serverless.yml` file what files we want, on what path, what methods, and let it upload using given AWS creds.

## Where are the environmental variables?

*Shh I have a secret repo*

So this app is deployed through GitHub actions, which allows for everything to be stored in secrets. I could've written a step that would take the contents of `secrets.env` and replace the existing `.env` file, but that means if I ever wanted to change the env contents, I'd have to change the secret, and the secret is hidden. So that leaves us in a situation where we have to store the env config somewhere so we have a record of it.

So why not just use the record instead?

So this repo has as its secret, a github token to clone out this config repo, which stores the config of not just this repo but all repos that I use for consistency. A custom action then overwrites the env file with a custom one with the proper environmental variables.
